#!/usr/bin/env python3
"""
Offline Q&A pipeline:
1) Ingest PDF / DOCX / TXT files from a folder
2) Chunk texts
3) Build embeddings with sentence-transformers -> FAISS index
4) Query: semantic retrieve top-k chunks -> run extractive QA pipeline (transformers)
"""

import os
import sys
import argparse
import pickle
from pathlib import Path
from typing import List, Dict, Tuple

from tqdm import tqdm
import numpy as np

# Document parsing
import PyPDF2
import docx

# Text processing
import nltk
nltk.download('punkt', quiet=True)
from nltk.tokenize import sent_tokenize

# Embeddings and search
from sentence_transformers import SentenceTransformer
import faiss

# QA model
from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering

# ----------------------------
# CONFIG - change these paths for offline use
# ----------------------------
# models must be available locally (download once), then set these paths to the folders
SENTENCE_TRANSFORMER_MODEL = "all-MiniLM-L6-v2"  # or local folder path of sentence-transformers model
QA_MODEL = "distilbert-base-uncased-distilled-squad"  # or local dir with QA model
# ----------------------------

# Helper: read PDF
def extract_text_from_pdf(file_path: str) -> str:
    text_chunks = []
    with open(file_path, "rb") as f:
        reader = PyPDF2.PdfReader(f)
        for page in reader.pages:
            text = page.extract_text()
            if text:
                text_chunks.append(text)
    return "\n".join(text_chunks)

# Helper: read docx
def extract_text_from_docx(file_path: str) -> str:
    doc = docx.Document(file_path)
    paragraphs = [p.text for p in doc.paragraphs if p.text]
    return "\n".join(paragraphs)

# Helper: read txt
def extract_text_from_txt(file_path: str) -> str:
    with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
        return f.read()

# Ingest directory
def ingest_documents(folder: str) -> Dict[str, str]:
    supported = (".pdf", ".docx", ".txt")
    docs = {}
    for root, _, files in os.walk(folder):
        for fname in files:
            if fname.lower().endswith(supported):
                path = os.path.join(root, fname)
                try:
                    if fname.lower().endswith(".pdf"):
                        text = extract_text_from_pdf(path)
                    elif fname.lower().endswith(".docx"):
                        text = extract_text_from_docx(path)
                    else:
                        text = extract_text_from_txt(path)
                    if text and len(text.strip()) > 0:
                        docs[path] = text
                except Exception as e:
                    print(f"Failed to read {path}: {e}", file=sys.stderr)
    return docs

# Chunking by sentences to produce overlapping chunks
def chunk_text(text: str, chunk_size: int = 500, overlap: int = 100) -> List[str]:
    sentences = sent_tokenize(text)
    chunks = []
    current = []
    current_len = 0
    for s in sentences:
        s_len = len(s.split())
        if current_len + s_len <= chunk_size:
            current.append(s)
            current_len += s_len
        else:
            chunks.append(" ".join(current))
            # start new chunk with overlap
            if overlap > 0:
                # keep last tokens/sentences to overlap
                # simplest: keep last sentence(s)
                current = current[-int(overlap/20):] if len(current) > 0 else []
                current_len = sum(len(x.split()) for x in current)
            else:
                current = []
                current_len = 0
            current.append(s)
            current_len += s_len
    if current:
        chunks.append(" ".join(current))
    # remove very short chunks
    return [c.strip() for c in chunks if len(c.strip()) > 50]

# Build index
def build_faiss_index(chunks: List[str], model_name: str, index_path: str = None) -> Tuple[faiss.IndexFlatIP, np.ndarray]:
    print("Loading sentence-transformer model:", model_name)
    embedder = SentenceTransformer(model_name)
    embeddings = embedder.encode(chunks, show_progress_bar=True, convert_to_numpy=True)
    # normalize for cosine via inner product
    faiss.normalize_L2(embeddings)
    dim = embeddings.shape[1]
    index = faiss.IndexFlatIP(dim)  # inner product on normalized vectors = cosine similarity
    index.add(embeddings)
    if index_path:
        faiss.write_index(index, index_path + ".index")
        # save embeddings and chunks metadata
        with open(index_path + ".meta.pkl", "wb") as f:
            pickle.dump({"chunks": chunks, "embeddings_shape": embeddings.shape}, f)
    return index, embeddings

# Save metadata only (chunks)
def save_meta(chunks: List[str], meta_path: str):
    with open(meta_path, "wb") as f:
        pickle.dump({"chunks": chunks}, f)

def load_meta(meta_path: str):
    with open(meta_path, "rb") as f:
        return pickle.load(f)["chunks"]

# Query pipeline
class QAPipeline:
    def __init__(self, sentence_model: str = SENTENCE_TRANSFORMER_MODEL, qa_model: str = QA_MODEL,
                 index: faiss.IndexFlatIP = None, chunks: List[str] = None):
        print("Loading embedding model:", sentence_model)
        self.embedder = SentenceTransformer(sentence_model)
        print("Loading QA model:", qa_model)
        # load tokenizer and model for pipeline
        self.qa_tokenizer = AutoTokenizer.from_pretrained(qa_model)
        self.qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model)
        self.qa_pipe = pipeline("question-answering", model=self.qa_model, tokenizer=self.qa_tokenizer)
        self.index = index
        self.chunks = chunks

    def retrieve(self, question: str, top_k: int = 5) -> List[Tuple[int, float]]:
        q_emb = self.embedder.encode([question], convert_to_numpy=True)
        faiss.normalize_L2(q_emb)
        D, I = self.index.search(q_emb, top_k)
        # D are similarities, I are indices
        results = []
        for idx, score in zip(I[0], D[0]):
            if idx == -1:
                continue
            results.append((int(idx), float(score)))
        return results

    def answer(self, question: str, top_k: int = 5) -> Dict:
        retrieved = self.retrieve(question, top_k=top_k)
        best_answer = {"answer": "", "score": -1.0, "context": "", "source_chunk_idx": None, "source_text_preview": ""}
        for idx, score in retrieved:
            context = self.chunks[idx]
            try:
                res = self.qa_pipe(question=question, context=context, top_k=1)
                # res may be dict or list
                if isinstance(res, list):
                    candidate = res[0]
                else:
                    candidate = res
                ans_text = candidate.get("answer", "")
                ans_score = float(candidate.get("score", 0.0))
                # Combine retrieval similarity and QA confidence optionally - here we choose QA score
                if ans_score > best_answer["score"]:
                    best_answer.update({
                        "answer": ans_text,
                        "score": ans_score,
                        "context": context,
                        "source_chunk_idx": idx,
                        "source_text_preview": context[:400]
                    })
            except Exception as e:
                print(f"QA model failed on chunk {idx}: {e}", file=sys.stderr)
        return {"question": question, "answer": best_answer["answer"], "confidence": best_answer["score"],
                "source_chunk_idx": best_answer["source_chunk_idx"], "source_preview": best_answer["source_text_preview"],
                "retrieved": retrieved}

# CLI helpers
def prepare_documents_and_index(data_folder: str, out_dir: str, chunk_size: int = 500, overlap: int = 100):
    os.makedirs(out_dir, exist_ok=True)
    print("Ingesting documents from:", data_folder)
    docs = ingest_documents(data_folder)
    print(f"Found {len(docs)} documents.")
    all_chunks = []
    doc_to_chunk_idx = {}
    for path, text in docs.items():
        chunks = chunk_text(text, chunk_size=chunk_size, overlap=overlap)
        start_idx = len(all_chunks)
        all_chunks.extend(chunks)
        doc_to_chunk_idx[path] = (start_idx, len(all_chunks))
    print(f"Total chunks: {len(all_chunks)}")
    # save chunks meta
    save_meta(all_chunks, os.path.join(out_dir, "chunks.meta.pkl"))
    # build faiss
    index_path = os.path.join(out_dir, "faiss_index")
    index, embeddings = build_faiss_index(all_chunks, SENTENCE_TRANSFORMER_MODEL, index_path=index_path)
    # save doc map
    with open(os.path.join(out_dir, "doc_to_chunk.pkl"), "wb") as f:
        pickle.dump(doc_to_chunk_idx, f)
    print("Index and metadata saved to", out_dir)
    return out_dir

def interactive_shell(out_dir: str):
    # load index and chunks
    idx_file = os.path.join(out_dir, "faiss_index.index")
    if not os.path.exists(idx_file):
        print("Index not found at", idx_file)
        return
    print("Loading index...")
    index = faiss.read_index(idx_file)
    chunks = load_meta(os.path.join(out_dir, "chunks.meta.pkl"))
    qa = QAPipeline(sentence_model=SENTENCE_TRANSFORMER_MODEL, qa_model=QA_MODEL, index=index, chunks=chunks)
    print("\nReady. Type your question (or 'exit').\n")
    while True:
        q = input("Question> ").strip()
        if not q:
            continue
        if q.lower() in ("exit", "quit"):
            break
        result = qa.answer(q, top_k=6)
        print("\nAnswer:", result["answer"])
        print("Confidence:", result["confidence"])
        print("Source preview:", result["source_preview"])
        print("Retrieved chunks (idx, sim):", result["retrieved"])
        print("-"*60)

# main
def main():
    parser = argparse.ArgumentParser(description="Offline Q&A pipeline")
    parser.add_argument("--data", type=str, help="Folder containing documents (pdf/docx/txt)", required=True)
    parser.add_argument("--out", type=str, help="Output folder for index/meta", default="qa_index")
    parser.add_argument("--chunk_size", type=int, default=500, help="Chunk size in words")
    parser.add_argument("--overlap", type=int, default=100, help="Overlap size in words")
    parser.add_argument("--build", action="store_true", help="Build index from data folder")
    parser.add_argument("--serve", action="store_true", help="Start interactive QA shell (requires built index)")
    args = parser.parse_args()

    if args.build:
        prepare_documents_and_index(args.data, args.out, chunk_size=args.chunk_size, overlap=args.overlap)

    if args.serve:
        interactive_shell(args.out)

if __name__ == "__main__":
    main()